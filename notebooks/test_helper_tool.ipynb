{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_1_text(topic):\n",
    "    return f\"\"\"\n",
    "    You are an expert medical doctor with a specialty in {topic}. You have specifically been the final arbiter for managing prior authorizations for decades.\n",
    "    I have a medical guideline from an insurance provider, written in markdown. The guidelines lay out all of the criteria by which a procedure or often multiple procedures might be considered \"medically necessary\". There are usually multiple paths that will yield this result.\n",
    "    You will be hiring a team of medical interns to help you perform more prior authorizations accurately. Your task is to make these medical guidelines as simple and easy to follow for your new team as possible.\n",
    "    To do this, you'll want to:\n",
    "    1. Read the guideline all the way through, carefully\n",
    "    2. Read the rationale for context, but do not create any questions based on it.\n",
    "    3. Break the guidelines out into sub guidelines based on the procedure being requested.\n",
    "    4. For each sub guideline, identify the questions that will comprise the checklists.\n",
    "    5. Guidelines sometimes highlight when a procedure is particularly useful. These are NOT requirements.Do not include questions in these cases.\n",
    "    6. Formulate a series of checklists, at least 1 for each procedure\n",
    "    7. Because these are checklists, you'll want to phrase your questions such that the desired answer is \"Yes\" or \"True\".\n",
    "    If your medical interns can answer \"Yes\" or \"True\" to all of the questions in any of your checklists, it means that the procedure is medically necessary, and they should approve it.\n",
    "    \"\"\"\n",
    "\n",
    "def get_step_3_text():\n",
    "    return \"\"\"\n",
    "  Now your job will be to create a flowchart of all of the checklists. But there are a few rules you must follow:\n",
    "  Rule 1: The first node must be \"start\" \n",
    "  Rule 2: The ONLY leaf node permitted is \"Procedure Medically Necessary\". (This is because I do not care about paths that lead to denial. They can be omitted) \n",
    "  Rule 3: Questions/Nodes may ONLY be connected with the Answer/Arc \"Yes\". Remember to convert the check list titles themselves into \"Yes\" questions, like the checklist items. \n",
    "  Rule 4: Convert Logical OR condition to parallel flows\n",
    "  Rule 4a: If you have a series of questions where ANY of them can be True/Yes, then they should be represented as parallel paths, not serial paths.\n",
    "  So if you have 3 procedures: \n",
    "  - Procedure 1\n",
    "  - Procedure 2\n",
    "  - Procedure 3 \n",
    "  The \"start\" node should first go to nodes in this form:\n",
    "  - \"Is Procedure 1 Requested\"\n",
    "  - \"Is Procedure 2 Requested\"\n",
    "  - \"Is Procedure 3 Requested\"\n",
    "  Rule 4b: If a checklist items has sub options, and the question asks if any of the following sub-options are true, you should create a separate, parallel flow for all sub options.\n",
    "  Rule 4c: If a checklist has sub options, and the question asks if the user has ANY of the following sub-options, you should create a separate, parallel flow for all sub options.\n",
    "  Rule 44: If a checklist has sub options, and the question asks if the user does NOT have ANY of the following sub-options, you should create a serial flow for all sub options.\n",
    "  Rule 5: Your response should include the mermaid flowchart inside <mermaid></mermaid> tags. \n",
    "  Rule 6: Be sure that the labels for all mermaid nosed are enclosed in quotes to avoid syntax errors\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(\n",
    "    api_key=os.getenv(\"CLAUDE_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import diskcache as dc\n",
    "\n",
    "cache = dc.Cache(\"data\")\n",
    "\n",
    "def hash_parameters(*args, **kwargs):\n",
    "    \"\"\"Create a hash of the function parameters.\"\"\"\n",
    "    kwargs.pop(\"response_format\", None)\n",
    "    params = json.dumps((args, kwargs))\n",
    "    return hashlib.md5(params.encode()).hexdigest()\n",
    "\n",
    "def cached(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        cache_kwargs = kwargs.copy()\n",
    "        if \"client\" in cache_kwargs:\n",
    "            cache_kwargs.pop(\"client\")\n",
    "        key = f\"{func.__name__}:{hash_parameters(*args, **cache_kwargs)}\"\n",
    "        if key in cache:\n",
    "            return cache[key]\n",
    "        result = func(*args, **kwargs)\n",
    "        cache[key] = result\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cached\n",
    "def get_claude_response(messages):\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "import requests\n",
    "\n",
    "\n",
    "# Implement backoff logic that will retry the request\n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,  # Exponential backoff\n",
    "    predicate=lambda r: r.status_code == 429,\n",
    "    value=lambda r: int(r.headers.get(\"Retry-After\")),\n",
    ")\n",
    "def get_gpt_response_api(prompt, model_name, endpoint, key, temperature):\n",
    "    headers = {\n",
    "        # \"authorization\": f\"Bearer {key}\",\n",
    "        \"api-key\": key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    modified_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\"results\"],\n",
    "        \"additionalProperties\": False,\n",
    "        \"properties\": {\n",
    "            \"results\": {\n",
    "                \"type\": \"object\",\n",
    "                    \"required\": [\"summary\", \"discussion\", \"answers\"],\n",
    "                    \"additionalProperties\": False,\n",
    "                    \"properties\": {\n",
    "                        \"summary\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Summary of the patient information focusing on aspects that are relevant to the question(s)\"\n",
    "                        },\n",
    "                        \"discussion\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"How does the provided information help or not help to answer the questions?\"\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"required\": [\"answer\", \"supporting evidence\"],\n",
    "                                \"additionalProperties\": False,\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"The correct answer choice among the multiple choices\"\n",
    "                                    },\n",
    "                                    \"supporting evidence\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"A direct quote from the patient information that supports the answer if one exists.\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # The data being sent in the request, including the schema and prompt\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        # \"logprobs\": True,\n",
    "        # \"top_logprobs\": 5,\n",
    "        # \"response_format\": {\n",
    "        #     \"type\": \"json_schema\",\n",
    "        #     \"json_schema\": {\n",
    "        #         \"name\": \"QuestionAnswers\",\n",
    "        #         \"strict\": True,\n",
    "        #         \"schema\": modified_schema\n",
    "        #     }\n",
    "        # }\n",
    "    }\n",
    "\n",
    "    # Send the request\n",
    "    print(f\"ENDPOINT: {endpoint}\")\n",
    "    response = requests.post(endpoint, headers=headers, json=data)\n",
    "    return response\n",
    "\n",
    "@cached\n",
    "def get_gpt_response(messages, model_name, endpoint, key):\n",
    "    try:\n",
    "        return openai.ChatCompletion.create(\n",
    "            api_key=key,\n",
    "            api_base=endpoint,\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            api_type=\"azure\",\n",
    "            engine=model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "        )\n",
    "    except openai.InvalidRequestError:\n",
    "        return {\"choices\": [{\"message\": {\"content\": \"Question 1: The request was deemed invalid and likely violated the content filters.\"}}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.0007 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\*'\n",
      "/tmp/ipykernel_104122/3541860543.py:2: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  markdown = \"#Other thoracic mass lesions Advanced imaging is considered medically necessary for diagnosis and management of **ANY** of the following findings or conditions: - Mediastinal mass (see separate indication for lymphadenopathy) - Pancoast tumor - Pleural mass - Thymoma - Benign tumors (pediatric only) **IMAGING STUDY** \\*\\*_ADULT_ \\*\\* - CT chest - MRI chest for evaluation of mediastinal and hilar masses when CT is insufficient for problem solving or for evaluation of chest wall extension in Pancoast tumor \\*\\*_PEDIATRIC_ \\*\\* - CT or MRI chest\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://guidelines.carelonmedicalbenefitsmanagement.com/imaging-of-the-chest-2024-04-14/\"\n",
    "markdown = \"#Other thoracic mass lesions Advanced imaging is considered medically necessary for diagnosis and management of **ANY** of the following findings or conditions: - Mediastinal mass (see separate indication for lymphadenopathy) - Pancoast tumor - Pleural mass - Thymoma - Benign tumors (pediatric only) **IMAGING STUDY** \\*\\*_ADULT_ \\*\\* - CT chest - MRI chest for evaluation of mediastinal and hilar masses when CT is insufficient for problem solving or for evaluation of chest wall extension in Pancoast tumor \\*\\*_PEDIATRIC_ \\*\\* - CT or MRI chest\"\n",
    "topic = \"Other thoracic mass lesions\"\n",
    "@cached\n",
    "def get_mermaid_from_gpt(topic, markdown):\n",
    "    message = [{\"role\": \"system\", \"content\": get_step_1_text(topic)},\n",
    "            {\"role\": \"user\", \"content\": markdown}]\n",
    "    response = get_gpt_response(message, \n",
    "                                    model_name=\"gpt-4o-latest\", \n",
    "                                    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_EAST2\"), \n",
    "                                    key=os.getenv(\"AZURE_OPENAI_API_KEY_EAST2\"), \n",
    "                                    # temperature=0.0\n",
    "                                    )\n",
    "    message.append({\"role\": \"system\",\n",
    "                    \"content\": response[\"choices\"][0][\"message\"][\"content\"]})\n",
    "    message.append({\"role\": \"user\",\n",
    "                    \"content\": get_step_3_text()})\n",
    "    second_response = get_gpt_response(message,\n",
    "                                    model_name=\"gpt-4o-latest\",\n",
    "                                    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_EAST2\"),\n",
    "                                    key=os.getenv(\"AZURE_OPENAI_API_KEY_EAST2\"))\n",
    "    return second_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "@cached\n",
    "def get_mermaid_from_claude(topic, markdown):\n",
    "    message = [{\"role\": \"user\", \"content\": get_step_1_text(topic)},\n",
    "            {\"role\": \"user\", \"content\": markdown}]\n",
    "    response = get_claude_response(message)\n",
    "    message.append({\"role\": \"assistant\",\n",
    "                    \"content\": response.content[0].text})\n",
    "    message.append({\"role\": \"user\",\n",
    "                    \"content\": get_step_3_text()})\n",
    "    second_response = get_claude_response(message)\n",
    "    return second_response.content[0].text\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "get_mermaid_from_claude(topic, markdown)\n",
    "print(f\"Finished in {(time.time()-start_time):.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying table: schema_migrations\n",
      "Copying table: guidelines\n",
      "Copying table: guideline_cpt\n",
      "Copying table: guideline_icd10\n",
      "Migration completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import scorcery\n",
    "import scorcery.create_local_overrides\n",
    "\n",
    "override_path = \"overrides.db\"\n",
    "if not os.path.isfile(override_path):\n",
    "    scorcery.create_local_overrides.migrate_postgres_to_sqlite(override_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import scorcery.final_db\n",
    "from peewee import SqliteDatabase\n",
    "import scorcery.final_db.guideline_repo\n",
    "import scorcery.flows\n",
    "\n",
    "\n",
    "target_db = SqliteDatabase(override_path)\n",
    "count = 0\n",
    "match_count = 0\n",
    "score_total = 0\n",
    "target_title = \"Penile, Vaginal, and Vulvar Cancers\"\n",
    "for target in scorcery.final_db.guideline_repo.GuidelineRepo.all(target_db):\n",
    "    count += 1\n",
    "    response = get_mermaid_from_claude(target.title, target.id)\n",
    "    try:\n",
    "        score = scorcery.flows.score_flows(target.flow, response)\n",
    "        # print(score)\n",
    "        score_total += score\n",
    "        match_count += 1\n",
    "    except Exception as e:\n",
    "        print(\"&\"*100)\n",
    "        print(f\"Error: {target.title} failed because: {e}\")\n",
    "        print(\"*\"*50)\n",
    "        print(target.flow)\n",
    "        print(\"*\"*50)\n",
    "        print(response)\n",
    "        print(\"&\"*100)\n",
    "    \n",
    "    # break\n",
    "\n",
    "print(count)\n",
    "print(match_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAUDE\n",
      "31.557373305161196\n",
      "0.7513660310752666\n",
      "0.7513660310752666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"CLAUDE\")\n",
    "print(score_total)\n",
    "print(score_total / count)\n",
    "print(score_total / match_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-latest\n",
      "27.37971372482997\n",
      "0.651897945829285\n",
      "0.7205187822323676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"gpt-4o-latest\")\n",
    "print(score_total)\n",
    "print(score_total / count)\n",
    "print(score_total / match_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiment-flows--m45dXvX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
